# Modeling Results

Five ML algorithms - Na√Øve Bayes, Logistic Regression, Random Forest, Bayes Net and KNN, are employed to construct different predictive models. All the five algorithms are available in Weka. Based on the final selected dataset in Section 4, the ML models are trained on 80% of the data and tested on the remaining 20%. Multiple metrics are used, including area under the ROC curve, Recall, Precision and F-measure, to evaluate the performance of these ML models. The details of the results are presented as follows.

## Naive Bayes
Naive Bayes classifier is a well-known algorithm for classification problems. It is based on Bayes' theorem with strong independence assumptions between the features. The detailed accuracy by class and the confusion matrix are shown in Figure 4(a), and the ROC curve is shown in Figure 4(b) where the area under ROC is 0.84. 


## Logistic Regression
the dependent variable, which maps any real value into a value between 0 and 1. Usually it works well for binary data, especially when the data is linearly separable. The detailed accuracy by class and the confusion matrix are shown in Figure 5(a), and the ROC curve is shown in Figure 5(b) where the area under ROC is 0.86.

## Random Forest
Random Forest is a very popular ML technique for classification problems. It belongs to a large class of algorithms based on decision trees.  It combines multiple decision trees to deliver an output, thus it is in fact an ensemble learning method. Batch size is an important hyperparameter for Random Forest. In this case, the tree size is set to ntree=100. The detailed accuracy by class and the confusion matrix are shown in Figure 6(a), and the ROC curve is shown in Figure 6(b) where the area under ROC is 0.95. As a comparison, the tree size is adjusted to ntree = 500 and it is found that the area under ROC does not change while Recall increase to 0.607 from 0.601. 

## Bayes Net
Bayes Net, also known as Bayesian Network, is a very powerful ML technique for classification problems. It is a probabilistic graphical model. Weka provides multiple options for Bayes Net algorithm. In this case, TAN and hill-climber are selected for use. The detailed accuracy by class and the confusion matrix are shown in Figure 7(a), 8(a), and the ROC curve is shown in Figure 7(b) and 8(b) where the area under ROC is 0.95 and 0.96 for using the TAN and Hill-Climber option, respectively. 

## KNN
KNN (K-Nearest Neighbors) is another very popular ML technique for classification problems. A basic assumption is that similar things exist in close proximity. KNN computes proximity of an individual point using the K nearest neighbors, and then determines which group this point belongs to. K is the parameter that can tune the performance of the KNN model. In this case, three K values are used: 3, 10 and 50. The detailed accuracy by class and the confusion matrix are shown in Figure 8(a-c). It is observed that the precision is 0.760 and 0.764 for K=3 and K=10, respectively. 

## Performance Summary 
As a summary, Table 3 shows the learning performance of the five different ML models. In terms of the area under ROC, all the models show good prediction performance. When taking into account all the assessment measures listed in the table, recall guarantees that the model recognizes a maximum number of actual deceased patients. On the other hand, high precision ensures the accuracy of predictions. The F-Measure indicates the equilibrium between these two metrics. The ROC Area provides a holistic perspective on the overall capability of the model to differentiate between individuals who survived and those who did not.

As a comparison, Bayes Net (with option Hill-Climber) classifier delivers the largest area under ROC and highest Recall, while Random Forest delivers the highest Precision value. For the remaining three algorithms, their performance is not good enough given that those Recall values are relatively low, especially for the KNN-based model which has the lowest Recall. 
